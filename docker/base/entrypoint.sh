#!/bin/bash
set -e

echo "ğŸš€ Starting AWS Ollama Platform Container (Dynamic Model Support)"
echo "Instance ID: ${INSTANCE_ID:-unknown}"
echo "User ID: ${USER_ID:-unknown}"
echo "Model Name: ${MODEL_NAME:-none}"
echo "Preload Model: ${PRELOAD_MODEL:-false}"

# Ollamaã‚µãƒ¼ãƒãƒ¼ã‚’ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã§èµ·å‹•
echo "ğŸ“¡ Starting Ollama server..."
ollama serve &
OLLAMA_PID=$!

# ã‚µãƒ¼ãƒãƒ¼ãŒèµ·å‹•ã™ã‚‹ã¾ã§å¾…æ©Ÿ
echo "â³ Waiting for Ollama server to start..."
for i in {1..30}; do
    if curl -f http://localhost:11434/api/tags >/dev/null 2>&1; then
        echo "âœ… Ollama server is ready!"
        break
    fi
    echo "   Attempt $i/30: Server not ready yet..."
    sleep 2
done

# ã‚µãƒ¼ãƒãƒ¼ãŒèµ·å‹•ã—ãªã‹ã£ãŸå ´åˆã¯ã‚¨ãƒ©ãƒ¼
if ! curl -f http://localhost:11434/api/tags >/dev/null 2>&1; then
    echo "âŒ Failed to start Ollama server"
    exit 1
fi

# å‹•çš„ãƒ¢ãƒ‡ãƒ«ç®¡ç†
if [ -n "$MODEL_NAME" ] && [ "$MODEL_NAME" != "none" ]; then
    echo "ğŸ¤– Managing model: $MODEL_NAME"
    
    # ãƒ¢ãƒ‡ãƒ«ç®¡ç†ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’å®Ÿè¡Œ
    if /app/model-manager.sh "$MODEL_NAME" "$PRELOAD_MODEL"; then
        echo "âœ… Model management completed successfully"
    else
        echo "âŒ Model management failed"
        echo "âš ï¸  Container will continue running, but model may not be available"
    fi
else
    echo "â„¹ï¸  No specific model requested, Ollama server ready for dynamic model loading"
fi

# åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«ã‚’è¡¨ç¤º
echo "ğŸ“‹ Available models:"
ollama list || echo "   No models available yet"

# ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±ã‚’è¡¨ç¤º
echo "ğŸ’» System Information:"
echo "   CPU cores: $(nproc)"
echo "   Memory: $(free -h | awk '/^Mem:/ {print $2}')"
echo "   Disk space: $(df -h / | awk 'NR==2 {print $4}')"

# GPUæƒ…å ±ï¼ˆåˆ©ç”¨å¯èƒ½ãªå ´åˆï¼‰
if command -v nvidia-smi >/dev/null 2>&1; then
    echo "ğŸ® GPU Information:"
    nvidia-smi --query-gpu=name,memory.total,memory.used --format=csv,noheader,nounits | \
        awk -F', ' '{printf "   GPU: %s, Memory: %s/%s MB\n", $1, $3, $2}'
fi

echo "ğŸ‰ Container initialization completed!"
echo "ğŸŒ Ollama API is available at http://0.0.0.0:11434"

# å‹•çš„ãƒ¢ãƒ‡ãƒ«æƒ…å ±ã®è¡¨ç¤º
if [ -n "$MODEL_NAME" ] && [ "$MODEL_NAME" != "none" ]; then
    echo "ğŸ”— Model-specific endpoint ready for: $MODEL_NAME"
    echo "ğŸ“ Example API call:"
    echo "   curl -X POST http://localhost:11434/api/generate \\"
    echo "        -H 'Content-Type: application/json' \\"
    echo "        -d '{\"model\":\"$MODEL_NAME\",\"prompt\":\"Hello\",\"stream\":false}'"
fi

# ã‚·ã‚°ãƒŠãƒ«ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°
cleanup() {
    echo "ğŸ›‘ Received shutdown signal"
    if [ -n "$OLLAMA_PID" ]; then
        echo "   Stopping Ollama server (PID: $OLLAMA_PID)..."
        kill -TERM "$OLLAMA_PID" 2>/dev/null || true
        wait "$OLLAMA_PID" 2>/dev/null || true
    fi
    echo "âœ… Cleanup completed"
    exit 0
}

trap cleanup SIGTERM SIGINT

# ãƒ•ã‚©ã‚¢ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã§Ollamaã‚µãƒ¼ãƒãƒ¼ã‚’ç¶™ç¶šå®Ÿè¡Œ
echo "ğŸ”„ Running in foreground mode..."
wait "$OLLAMA_PID"

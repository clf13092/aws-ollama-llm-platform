#!/bin/bash
set -e

echo "ğŸš€ Starting AWS Ollama Platform Container"
echo "Instance ID: ${INSTANCE_ID:-unknown}"
echo "User ID: ${USER_ID:-unknown}"
echo "Model Name: ${MODEL_NAME:-none}"
echo "Preload Model: ${PRELOAD_MODEL:-false}"

# Ollamaã‚µãƒ¼ãƒãƒ¼ã‚’ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã§èµ·å‹•
echo "ğŸ“¡ Starting Ollama server..."
ollama serve &
OLLAMA_PID=$!

# ã‚µãƒ¼ãƒãƒ¼ãŒèµ·å‹•ã™ã‚‹ã¾ã§å¾…æ©Ÿ
echo "â³ Waiting for Ollama server to start..."
for i in {1..30}; do
    if curl -f http://localhost:11434/api/tags >/dev/null 2>&1; then
        echo "âœ… Ollama server is ready!"
        break
    fi
    echo "   Attempt $i/30: Server not ready yet..."
    sleep 2
done

# ã‚µãƒ¼ãƒãƒ¼ãŒèµ·å‹•ã—ãªã‹ã£ãŸå ´åˆã¯ã‚¨ãƒ©ãƒ¼
if ! curl -f http://localhost:11434/api/tags >/dev/null 2>&1; then
    echo "âŒ Failed to start Ollama server"
    exit 1
fi

# ãƒ¢ãƒ‡ãƒ«ã®ãƒ—ãƒªãƒ­ãƒ¼ãƒ‰
if [ "$PRELOAD_MODEL" = "true" ] && [ -n "$MODEL_NAME" ]; then
    echo "ğŸ“¦ Preloading model: $MODEL_NAME"
    
    # ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
    echo "   Downloading model..."
    if ollama pull "$MODEL_NAME"; then
        echo "âœ… Model downloaded successfully"
        
        # ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ¡ãƒ¢ãƒªã«ãƒ­ãƒ¼ãƒ‰ï¼ˆå°ã•ãªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã§å®Ÿè¡Œï¼‰
        echo "   Loading model into memory..."
        if curl -X POST http://localhost:11434/api/generate \
            -H "Content-Type: application/json" \
            -d "{\"model\":\"$MODEL_NAME\",\"prompt\":\"Hello\",\"stream\":false}" \
            >/dev/null 2>&1; then
            echo "âœ… Model loaded into memory"
        else
            echo "âš ï¸  Warning: Failed to load model into memory, but continuing..."
        fi
    else
        echo "âŒ Failed to download model: $MODEL_NAME"
        echo "   Available models:"
        ollama list || echo "   No models available"
        # ãƒ¢ãƒ‡ãƒ«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã«å¤±æ•—ã—ã¦ã‚‚ã‚µãƒ¼ãƒãƒ¼ã¯ç¶™ç¶š
    fi
else
    echo "â„¹ï¸  No model preloading requested"
fi

# åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«ã‚’è¡¨ç¤º
echo "ğŸ“‹ Available models:"
ollama list || echo "   No models available"

# ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±ã‚’è¡¨ç¤º
echo "ğŸ’» System Information:"
echo "   CPU cores: $(nproc)"
echo "   Memory: $(free -h | awk '/^Mem:/ {print $2}')"
echo "   Disk space: $(df -h / | awk 'NR==2 {print $4}')"

# GPUæƒ…å ±ï¼ˆåˆ©ç”¨å¯èƒ½ãªå ´åˆï¼‰
if command -v nvidia-smi >/dev/null 2>&1; then
    echo "ğŸ® GPU Information:"
    nvidia-smi --query-gpu=name,memory.total,memory.used --format=csv,noheader,nounits | \
        awk -F', ' '{printf "   GPU: %s, Memory: %s/%s MB\n", $1, $3, $2}'
fi

echo "ğŸ‰ Container initialization completed!"
echo "ğŸŒ Ollama API is available at http://0.0.0.0:11434"

# ã‚·ã‚°ãƒŠãƒ«ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°
cleanup() {
    echo "ğŸ›‘ Received shutdown signal"
    if [ -n "$OLLAMA_PID" ]; then
        echo "   Stopping Ollama server (PID: $OLLAMA_PID)..."
        kill -TERM "$OLLAMA_PID" 2>/dev/null || true
        wait "$OLLAMA_PID" 2>/dev/null || true
    fi
    echo "âœ… Cleanup completed"
    exit 0
}

trap cleanup SIGTERM SIGINT

# ãƒ•ã‚©ã‚¢ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã§Ollamaã‚µãƒ¼ãƒãƒ¼ã‚’ç¶™ç¶šå®Ÿè¡Œ
echo "ğŸ”„ Running in foreground mode..."
wait "$OLLAMA_PID"
